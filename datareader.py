import numpy as np, tensorflow as tf
import os
import itertools
from functools import reduce
import random

class txtdatareader():
    def __init__(self, fileloc,trainratio):
        self.traindataset, self.testdataset = self.readdata(fileloc,trainratio)
        self.trainsize = len(self.traindataset)
        self.testsize  = len(self.testdataset)
        self.maxlen    = max(reduce(lambda x,y:max(x,y), map(lambda x: len(x[0]),self.traindataset)),\
                             reduce(lambda x,y:max(x,y), map(lambda x: len(x[0]),self.testdataset)))
        self.minlen    = min(reduce(lambda x,y:min(x,y), map(lambda x: len(x[0]),self.traindataset)),\
                             reduce(lambda x,y:min(x,y), map(lambda x: len(x[0]),self.testdataset)))
        self.cursor    = 0
        self.testcursor = 0
        self.trainepochs    = 0
        self.testepochs     = 0
    def get_all_name(self, location):
        datasetlist = os.listdir(location)
        #datasetlist.sort()
        return datasetlist

    def readdata(self,fileloc,trainratio):
        samplelist = self.get_all_name(fileloc)
        trainset = []
        testset  = []
        for sample in samplelist:
            dataset = []
            print(fileloc + sample)
            try:
                with open(fileloc + sample) as readfile:
                    for line in readfile:
                        _, data = line.strip('\n').split('\t')
                        data = list(itertools.chain.from_iterable(eval(data)))
                        if sample.lower().startswith('benign'.lower()):
                            dataset.append((np.array(data), 1))
                        else:
                            dataset.append((np.array(data), 0))
                    print("file:",sample,"  samples:", len(dataset))
            except:
                print("sample %s is not found"%sample)
            trainset += dataset[:int(len(dataset)*trainratio)]
            testset  += dataset[int(len(dataset)*trainratio):]
        return trainset, testset

    def padding(self,array):
        array = np.lib.pad(array, (self.maxlen-len(array),0), 'constant', constant_values=(0))
        return array

    def shuffle(self,Type = "train"):
        if Type == "train":
            random.shuffle(self.traindataset)
        else:
            random.shuffle(self.testdataset)
        self.cursor = 0

    def next_batch(self, n,Type = "train"):
        if Type == "train":
            trainlen = len(self.traindataset)
            if self.cursor+n-1 > self.trainsize:
                self.trainepochs += 1
                self.shuffle(Type)
            result = self.traindataset[min(trainlen-n,self.cursor):min(trainlen, self.cursor+n)]
            self.cursor += n
            data = np.array(list(map(lambda x:self.padding(x[0]),result)))
            label = np.array(list(map(lambda x:x[1], result)))
            slen = np.array(list(map(lambda x: len(x[0]), result)))
            return data, label, slen
        else:
            testlen = len(self.testdataset)
            if self.testcursor+n-1 > self.testsize:
                self.testepochs += 1
                self.shuffle(Type)
            result = self.testdataset[min(testlen-n,self.testcursor):min(testlen, self.testcursor+n)]
            self.testcursor += n
            data = np.array(list(map(lambda x:self.padding(x[0]),result)))
            label = np.array(list(map(lambda x:x[1], result)))
            slen = np.array(list(map(lambda x: len(x[0]), result)))
            return data, label, slen

if __name__ == '__main__':
    test = txtdatareader("../data/sampledata/",trainratio=0.8)
    print(test.trainsize+test.testsize)
    data, label, slen = test.next_batch(3,Type="train")
    print(slen)

    '''
    test = PaddedDataIterator("../data/datasets/",trainratio=0.8)
    print(test.maxlen)
    print(test.minlen)
    for i in range(len(test.traindataset)):
        print(test.traindataset[i][1])	
    '''
